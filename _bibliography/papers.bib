---
---
@article{journals/corr/abs-2402-07860,
  author       = {Jecmen, Steven and Shah, Nihar B. and Fang, Fei and Akoglu, Leman},
  title        = {On the Detection of Reviewer-Author Collusion Rings From Paper Bidding},
  journal      = {Preprint},
  volume       = {abs/2402.07860},
  year         = {2024},
  url          = {https://arxiv.org/abs/2402.07860},
  eprinttype    = {arXiv},
  eprint       = {2402.07860},
  arxiv = {2402.07860},
  abbr = {arXiv},
  abstract = {A major threat to the peer-review systems of computer science conferences is the existence of "collusion rings" between reviewers. In such collusion rings, reviewers who have also submitted their own papers to the conference work together to manipulate the conference's paper assignment, with the aim of being assigned to review each other's papers. The most straightforward way that colluding reviewers can manipulate the paper assignment is by indicating their interest in each other's papers through strategic paper bidding. One potential approach to solve this important problem would be to detect the colluding reviewers from their manipulated bids, after which the conference can take appropriate action. While prior work has developed effective techniques to detect other kinds of fraud, no research has yet established that detecting collusion rings is even possible. In this work, we tackle the question of whether it is feasible to detect collusion rings from the paper bidding. To answer this question, we conduct empirical analysis of two realistic conference bidding datasets, including evaluations of existing algorithms for fraud detection in other applications. We find that collusion rings can achieve considerable success at manipulating the paper assignment while remaining hidden from detection: for example, in one dataset, undetected colluders are able to achieve assignment to up to 30\% of the papers authored by other colluders. In addition, when 10 colluders bid on all of each other's papers, no detection algorithm outputs a group of reviewers with more than 31\% overlap with the true colluders. These results suggest that collusion cannot be effectively detected from the bidding using popular existing tools, demonstrating the need to develop more complex detection algorithms as well as those that leverage additional metadata (e.g., reviewer-paper text-similarity scores).},
}

@inproceedings{journals/corr/abs-2310-05995,
  author       = {Xu, Yixuan Even and Jecmen, Steven and Song, Zimeng and Fang, Fei},
  title        = {A One-Size-Fits-All Approach to Improving Randomness in Paper Assignment},
  booktitle = {{NeurIPS}},
  %volume       = {abs/2310.05995},
  year         = {2023},
  url          = {https://arxiv.org/abs/2310.05995},
  eprinttype    = {arXiv},
  eprint       = {2310.05995},
  arxiv = {2310.05995},
  abbr = {NeurIPS 2023},
  abstract = {The assignment of papers to reviewers is a crucial part of the peer review processes of large publication venues, where organizers (e.g., conference program chairs) rely on algorithms to perform automated paper assignment. As such, a major challenge for the organizers of these processes is to specify paper assignment algorithms that find appropriate assignments with respect to various desiderata. Although the main objective when choosing a good paper assignment is to maximize the expertise of each reviewer for their assigned papers, several other considerations make introducing randomization into the paper assignment desirable: robustness to malicious behavior, the ability to evaluate alternative paper assignments, reviewer diversity, and reviewer anonymity. However, it is unclear in what way one should randomize the paper assignment in order to best satisfy all of these considerations simultaneously. In this work, we present a practical, one-size-fits-all method for randomized paper assignment intended to perform well across different motivations for randomness. We show theoretically and experimentally that our method outperforms currently-deployed methods for randomized paper assignment on several intuitive randomness metrics, demonstrating that the randomized assignments produced by our method are general-purpose.},
  pdf = {onesize.pdf}
}

@article{journals/corr/abs-2307-05443,
  author       = {Liu, Ryan and Jecmen, Steven and Conitzer, Vincent and Fang, Fei and Shah, Nihar B.},
  title        = {Testing for Reviewer Anchoring in Peer Review: {A} Randomized Controlled
                  Trial},
  journal      = {Preprint},
  volume       = {abs/2307.05443},
  year         = {2023},
  url          = {https://arXiv.org/abs/2307.05443},
  eprinttype    = {arXiv},
  eprint       = {2307.05443},
  arxiv = {2307.05443},
  abbr = {arXiv},
  abstract = {Peer review frequently follows a process where reviewers first provide initial reviews, authors respond to these reviews, then reviewers update their reviews based on the authors' response. There is mixed evidence regarding whether this process is useful, including frequent anecdotal complaints that reviewers insufficiently update their scores. In this study, we aim to investigate whether reviewers anchor to their original scores when updating their reviews, which serves as a potential explanation for the lack of updates in reviewer scores. We design a novel randomized controlled trial to test if reviewers exhibit anchoring. In the experimental condition, participants initially see a flawed version of a paper that is later corrected, while in the control condition, participants only see the correct version. We take various measures to ensure that in the absence of anchoring, reviewers in the experimental group should revise their scores to be identically distributed to the scores from the control group. Furthermore, we construct the reviewed paper to maximize the difference between the flawed and corrected versions, and employ deception to hide the true experiment purpose. Our randomized controlled trial consists of 108 researchers as participants. First, we find that our intervention was successful at creating a difference in perceived paper quality between the flawed and corrected versions: Using a permutation test with the Mann-Whitney U statistic, we find that the experimental group's initial scores are lower than the control group's scores in both the Evaluation category (Vargha-Delaney A=0.64, p=0.0096) and Overall score (A=0.59, p=0.058). Next, we test for anchoring by comparing the experimental group's revised scores with the control group's scores. We find no significant evidence of anchoring in either the Overall (A=0.50, p=0.61) or Evaluation category (A=0.49, p=0.61).},
}

@inproceedings{journals/corr/abs-2305-17339,
  author       = {Saveski, Martin and Jecmen, Steven and Shah, Nihar B. and Ugander, Johan},
  title        = {Counterfactual Evaluation of Peer-Review Assignment Policies},
  booktitle = {{NeurIPS}},
  %volume       = {abs/2305.17339},
  year         = {2023},
  url          = {https://arXiv.org/abs/2305.17339},
  eprinttype    = {arXiv},
  eprint       = {2305.17339},
  arxiv = {2305.17339},
  %alsoin = {Peer Review Congress 2022 (Abstract)},
  abbr = {NeurIPS 2023},
  abstract = {Peer review assignment algorithms aim to match research papers to suitable expert reviewers, working to maximize the quality of the resulting reviews. A key challenge in designing effective assignment policies is evaluating how changes to the assignment algorithm map to changes in review quality. In this work, we leverage recently proposed policies that introduce randomness in peer-review assignment--in order to mitigate fraud--as a valuable opportunity to evaluate counterfactual assignment policies. Specifically, we exploit how such randomized assignments provide a positive probability of observing the reviews of many assignment policies of interest. To address challenges in applying standard off-policy evaluation methods, such as violations of positivity, we introduce novel methods for partial identification based on monotonicity and Lipschitz smoothness assumptions for the mapping between reviewer-paper covariates and outcomes. We apply our methods to peer-review data from two computer science venues: the TPDP'21 workshop (95 papers and 35 reviewers) and the AAAI'22 conference (8,450 papers and 3,145 reviewers). We consider estimates of (i) the effect on review quality when changing weights in the assignment algorithm, e.g., weighting reviewers' bids vs. textual similarity (between the review's past papers and the submission), and (ii) the "cost of randomization", capturing the difference in expected quality between the perturbed and unperturbed optimal match. We find that placing higher weight on text similarity results in higher review quality and that introducing randomization in the reviewer-paper assignment only marginally reduces the review quality. Our methods for partial identification may be of independent interest, while our off-policy approach can likely find use evaluating a broad class of algorithmic matching systems.},
  pdf = {counterfactual.pdf}
}

@inproceedings{journals/corr/abs-2207-02303,
  author    = {Jecmen, Steven and Yoon, Minji and Conitzer, Vincent and Shah, Nihar B. and Fang, Fei},
  title     = {A Dataset on Malicious Paper Bidding in Peer Review},
  %journal   = {Preprint},
  booktitle = {{WWW}},
  %volume    = {abs/2207.02303},
  year      = {2023},
  url       = {https://arXiv.org/abs/2207.02303},
  eprinttype = {arXiv},
  eprint    = {2207.02303},
  arxiv = {2207.02303},
  abbr = {WWW 2023},
  abstract = {In conference peer review, reviewers are often asked to provide "bids" on each submitted paper that express their interest in reviewing that paper. A paper assignment algorithm then uses these bids (along with other data) to compute a high-quality assignment of reviewers to papers. However, this process has been exploited by malicious reviewers who strategically bid in order to unethically manipulate the paper assignment, crucially undermining the peer review process. For example, these reviewers may aim to get assigned to a friend's paper as part of a quid-pro-quo deal. A critical impediment towards creating and evaluating methods to mitigate this issue is the lack of any publicly-available data on malicious paper bidding. In this work, we collect and publicly release a novel dataset to fill this gap, collected from a mock conference activity where participants were instructed to bid either honestly or maliciously. We further provide a descriptive analysis of the bidding behavior, including our categorization of different strategies employed by participants. Finally, we evaluate the ability of each strategy to manipulate the assignment, and also evaluate the performance of some simple algorithms meant to detect malicious bidding. The performance of these detection algorithms can be taken as a baseline for future research on detecting malicious bidding.},
  pdf = {dataset.pdf},
}

@inproceedings{journals/corr/abs-2207-11315,
  author    = {Jecmen, Steven and Shah, Nihar B. and Fang, Fei and Conitzer, Vincent},
  title     = {Tradeoffs in Preventing Manipulation in Paper Bidding for Reviewer
               Assignment},
  %journal   = {arXiv preprint},
  booktitle = {{ML Evaluation Standards Workshop at ICLR}},
  %volume    = {abs/2207.11315},
  year      = {2022},
  url       = {https://arXiv.org/abs/2207.11315},
  eprinttype = {arXiv},
  eprint    = {2207.11315},
  arxiv = {2207.11315},
  abbr = {Workshop},
  abstract = {Many conferences rely on paper bidding as a key component of their reviewer assignment procedure. These bids are then taken into account when assigning reviewers to help ensure that each reviewer is assigned to suitable papers. However, despite the benefits of using bids, reliance on paper bidding can allow malicious reviewers to manipulate the paper assignment for unethical purposes (e.g., getting assigned to a friend's paper). Several different approaches to preventing this manipulation have been proposed and deployed. In this paper, we enumerate certain desirable properties that algorithms for addressing bid manipulation should satisfy. We then offer a high-level analysis of various approaches along with directions for future investigation.},
  award={Outstanding Paper Award}
}


@inproceedings{journals/corr/abs-2201-10631,
  author    = {Dhull, Komal and Jecmen, Steven and Kothari, Pravesh and Shah, Nihar B.},
  title     = {Strategyproofing Peer Assessment via Partitioning: The Price in Terms of Evaluators' Expertise},
  booktitle = {{HCOMP}},
  %alsoin = {{Games, Agents, and Incentives Workshop at AAMAS 2022}; {Peer Review Congress 2022 (Abstract)}},
  year      = {2022},
  url       = {https://arxiv.org/abs/2201.10631},
  eprinttype = {arXiv},
  eprint    = {2201.10631},
  arxiv     = {2201.10631},
  abbr		= {HCOMP 2022},
  abstract  = {Strategic behavior is a fundamental problem in a variety of real-world applications that require some form of peer assessment, such as peer grading of homeworks, grant proposal review, conference peer review of scientific papers, and peer assessment of employees in organizations. Since an individual's own work is in competition with the submissions they are evaluating, they may provide dishonest evaluations to increase the relative standing of their own submission. This issue is typically addressed by partitioning the individuals and assigning them to evaluate the work of only those from different subsets. Although this method ensures strategyproofness, each submission may require a different type of expertise for effective evaluation. In this paper, we focus on finding an assignment of evaluators to submissions that maximizes assigned evaluators' expertise subject to the constraint of strategyproofness. We analyze the price of strategyproofness: that is, the amount of compromise on the assigned evaluators' expertise required in order to get strategyproofness. We establish several polynomial-time algorithms for strategyproof assignment along with assignment-quality guarantees. Finally, we evaluate the methods on a dataset from conference peer review.},
  pdf = {strategyproofing.pdf},
}

@inproceedings{journals/corr/abs-2108-06371,
  abbr		= {HCOMP 2022},
  author    = {Jecmen, Steven and Zhang, Hanrui and Liu, Ryan and Fang, Fei and Conitzer, Vincent and Shah, Nihar B.},
  title     = {Near-Optimal Reviewer Splitting in Two-Phase Paper Reviewing and Conference
               Experiment Design},
  booktitle = {{HCOMP}},
  alsoin   =  {AAMAS 2022 (Extended Abstract)},
  year      = {2022},
  url       = {https://arxiv.org/abs/2108.06371},
  eprinttype = {arXiv},
  eprint    = {2108.06371},
  arxiv 	= {2108.06371},
  abstract	= {Many scientific conferences employ a two-phase paper review process, where some papers are assigned additional reviewers after the initial reviews are submitted. Many conferences also design and run experiments on their paper review process, where some papers are assigned reviewers who provide reviews under an experimental condition. In this paper, we consider the question: how should reviewers be divided between phases or conditions in order to maximize total assignment similarity? We make several contributions towards answering this question. First, we prove that when the set of papers requiring additional review is unknown, a simplified variant of this problem is NP-hard. Second, we empirically show that across several datasets pertaining to real conference data, dividing reviewers between phases/conditions uniformly at random allows an assignment that is nearly as good as the oracle optimal assignment. This uniformly random choice is practical for both the two-phase and conference experiment design settings. Third, we provide explanations of this phenomenon by providing theoretical bounds on the suboptimality of this random strategy under certain natural conditions. From these easily-interpretable conditions, we provide actionable insights to conference program chairs about whether a random reviewer split is suitable for their conference.},
  award={Best Paper Honorable Mention},
  pdf = {reviewer_splitting.pdf},
}

@inproceedings{JecmenZLSCF20,
  abbr		= {NeurIPS 2020},
  author    = {Jecmen, Steven and Zhang, Hanrui and Liu, Ryan and Shah, Nihar B. and Conitzer, Vincent and Fang, Fei},
  title     = {Mitigating Manipulation in Peer Review via Randomized Reviewer Assignments},
  booktitle = {{NeurIPS}},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.16437},
  pdf		= {mitigating_manipulation.pdf},
  arxiv		= {2006.16437},
  abstract	= {We consider three important challenges in conference peer review: (i) reviewers maliciously attempting to get assigned to certain papers to provide positive reviews, possibly as part of quid-pro-quo arrangements with the authors; (ii) "torpedo reviewing," where reviewers deliberately attempt to get assigned to certain papers that they dislike in order to reject them; (iii) reviewer de-anonymization on release of the similarities and the reviewer-assignment code. On the conceptual front, we identify connections between these three problems and present a framework that brings all these challenges under a common umbrella. We then present a (randomized) algorithm for reviewer assignment that can optimally solve the reviewer-assignment problem under any given constraints on the probability of assignment for any reviewer-paper pair. We further consider the problem of restricting the joint probability that certain suspect pairs of reviewers are assigned to certain papers, and show that this problem is NP-hard for arbitrary constraints on these joint probabilities but efficiently solvable for a practical special case. Finally, we experimentally evaluate our algorithms on datasets from past conferences, where we observe that they can limit the chance that any malicious reviewer gets assigned to their desired paper to 50% while producing assignments with over 90% of the total optimal similarity. Our algorithms still achieve this similarity while also preventing reviewers with close associations from being assigned to the same paper.},
  notes={Implemented at OpenReview.net. Deployed for paper assignments in various venues, including AAAI 2022-2023 and KDD 2023.},
  %alsoin={{Incentives in Machine Learning Workshop at ICML} 2020; {Games, Agents, and Incentives Workshop at AAMAS} 2021; {International Workshop on Computational Social Choice} 2021}
}

@inproceedings{JecmenSLT20,
  abbr		= {AAAI 2020},
  author    = {Jecmen, Steven and Sinha, Arunesh and Li, Zun and Tran{-}Thanh, Long},
  title     = {Bounding Regret in Empirical Games},
  booktitle = {{AAAI}},
  pages     = {4280--4287},
  publisher = {{AAAI} Press},
  year      = {2020},
  url       = {https://aaai.org/ojs/index.php/AAAI/article/view/5851},
  pdf		= {bounding_regret.pdf},
  abstract	= {Empirical game-theoretic analysis refers to a set of models and techniques for solving large-scale games. However, there is a lack of a quantitative guarantee about the quality of output approximate Nash equilibria (NE). A natural quantitative guarantee for such an approximate NE is the regret in the game (i.e. the best deviation gain). We formulate this deviation gain computation as a multi-armed bandit problem, with a new optimization goal unlike those studied in prior work. We propose an efficient algorithm Super-Arm UCB (SAUCB) for the problem and a number of variants. We present sample complexity results as well as extensive experiments that show the better performance of SAUCB compared to several baselines.},
  %alsoin = {{Exploration in Reinforcement Learning Workshop at ICML} 2018}
}

